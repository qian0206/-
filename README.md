# -
   在现代网络平台上，推荐系统对于让用户参与到个性化内容中来起着至关重要的作用。深度学习使许多研究领域发生了革命性的变化，最近人们对将其应用于协同过滤(CF)的兴趣激增。然而，现有的方法组成了包含潜在因素模型的深度学习体系结构，忽略了CF模型的主要类、邻域或基于内存的方法。利用潜在因子模型的全局结构和局部邻域结构的优势，提出了协同记忆网络(CMN)，这是一种将两类CF模型结合起来的深层体系结构。在记忆网络成功的激励下，我们将记忆构件与神经注意机制融合为邻域构件。记忆模块中与用户和项记忆的关联寻址方案，结合神经注意机制，对复杂的用户-项关系进行编码，以学习用户-项特定的邻域。最后，输出模块与用户和项目内存共同利用邻域来生成排名分数。将多个记忆模块叠加在一起可以产生更深层次的体系结构，从而捕获日益复杂的用户-项目关系。此外，我们还展示了CMN组件、内存网络和三类CF模型之间的强大连接。综合实验结果表明，CMN在三个公共数据集上的有效性优于竞争基线。注意力权重的定性可视化提供了对模型srecommendation过程的深入了解，并表明存在更高阶的交互
   信息系统→推荐系统;•计算方法→神经网络
   关键词：深度学习;记忆网络;协同过滤
   一介绍：
   在信息爆炸的时代，推荐系统对于保持用户参与并满足个性化推荐至关重要。用户希望在现代商业、娱乐和社交媒体平台上提供个性化的内容，但现有的用户条目交互和模型容量限制了推荐的有效性。利用更高阶或阶推理的能力可能有助于缓解稀疏性问题。协同过滤(CF)是一种流行且成功的技术，它假定相似的用户会消费相似的商品，从而从过去的交互(例如点击、评级、购买)中建立用户和商品之间的关联。
   CF一般可以分为三类:基于内存或邻域的方法、潜在因素模型和混合模型[17,26]。基于内存或邻域的方法通过根据以前的交互历史确定similar用户或项目的组或邻域来形成推荐。这些模型(如itemKnearest neighbour (KNN))在Amazon[21]的生产系统中取得了成功。潜在因素模型(如矩阵分解)将每个用户和项投射到一个公共的低维空间中，捕获潜在的关系。邻域方法捕捉局部结构，但通常忽略了大多数可用的评级，因为从两个或[17]项之间的反馈交集中选择最多的观测值。另一方面，潜在因素模型捕获了用户和项目关系的整体全局结构，但通常忽略了一些强关联的存在。基于局部邻域的因子模型和全局因子模型之间的弱点导致了混合模型的开发，如SVD++[17]，以及综合了基于邻域的方法和潜在因子模型的因子分解机器[24]，从而丰富了预测能力。
   近年来，深度学习在计算机视觉[9]、问答[18、30、35、39]、学习程序[8]、机器翻译[1]等多个领域取得了长足的进步，取得了计算机视觉[9]的最新表现。深度学习方法在推荐系统中的成功集成证明了复合非线性变换相对于传统线性模型[40]的显著优势。然而，现有的复合架构合并了latentfactor模型，忽略了以非线性方式集成基于邻居的ap-proaches。因此，我们建议使用内存网络来表示基于邻居的组件[30,35]，以捕获用户和项之间更高阶的复杂关系。外部记忆允许编码丰富的特征表示，而神经注意机制则推断出用户来自社区的特定关注
    我们提出了一种统一的混合模型，该模型利用了记忆网络和隐式反馈神经注意机制的最新进展。内存组件允许读写操作在内部内存中编码复杂的用户和项关系。关联寻址模式充当基于自适应用户项状态查找语义相似的用户的最近邻模型。神经注意机制将更高的权重放在特定的用户子集上，这些用户具有相似的偏好，形成了一个共同的邻域和-mary。最后，通过局部邻域总结与全局潜在因子1之间的非线性相互作用，得到了排序得分。将多个内存组件叠加在一起，可以让模型推断出更精确的邻域，从而进一步提高性能。
    我们的主要贡献可以概括如下:
    1.基于记忆网络的成功经验，提出了一种基于协同过滤的协同记忆网络(CMN)。CMN通过外部记忆和神经注意机制得到增强。内存模块的关联寻址方案作为识别相似用户的最近邻模型。该注意机制根据特定的用户和项目学习用户邻域的自适应非线性加权。输出模块利用自适应邻域状态与用户和项目记忆之间的非线性相互作用，推导出推荐值。        
    2.揭示了CMN与两类重要的协同过滤模型之间的关系:潜在因素模型和基于邻域的相似性模型。进一步，我们揭示了非线性积分的优点，融合了两种类型的模型得到一个混合模型。
    3.在三个公共数据集上的综合实验证明了CMN在七个竞争基线上的有效性。多个实验配置证实了内存模块2的额外好处。
    4.注意力权重的定性可视化提供了对内存组件的洞察，为更深层次的体系结构捕获更高阶的复杂交互提供了支持证据
    二我们的工作
    2.1深度学习在推荐系统中
       1.最近，将深度学习应用于康复系统的兴趣激增。在早期的工作中，Salakhut-dinov等人通过应用双层限制玻尔兹曼机模型表级电影评分的[27]地址协同过滤。自动编码器一直是推荐系统深度学习体系结构的一种流行选择[20,28,333,37,41]。它作为一个非线性分解的评级矩阵1我们使用术语用户/项目潜在因素，记忆和嵌入互换。2源代码:http://github.com/tebesu/collaborativememorynetworkreplace传统的线性内积。例如，AutoRec[28]通过重构后的自动编码器对评级矩阵进行分解，直接预测评级，从而获得众多基准数据集上的竞争结果。协同去噪自动编码器(CDAE)[37]地址的top-nrecommendation通过在一个自动编码器演示CDAE中加入一个特定于用户的偏置，可以看作是许多现有协同过滤方法的推广。Li等人采用边缘去噪的自动编码器，降低了与深度学习相关的计算成本。使用两个自动编码器，一个用于项目内容，另一个用于连接用户和项目潜在因素的用户内容。AutoSVD++[41]对原有的SVD++模型进行了扩展，使用对比式自动编码器捕获辅助项信息。一个分层贝叶斯模型[33]桥矩阵分解与最深层的堆叠去噪自动编码器利用项目内容的过程。
       2.通过联合学习矩阵分解和前馈神经网络来解决隐式反馈问题。然后在最终输出之前将输出连接起来，以产生潜在因素和非线性因素之间的交互作用。Ebesu和Fang[4]通过紧密耦合深度神经网络(DNN)来解决项目冷启动问题，并通过成对矩阵分解来分解评分。DNN为新项目构建了项目内容和项目潜在因素的健壮表示。Cheng等人联合训练alogistic regression和DNN，利用深度学习的泛化方面和广义线性模型的特异性，在谷歌Play store中推荐移动应用程序。
       3.注意机制最近在推荐-修复系统中得到了探索。龚和张[6]在CNN上进行了话题标签推荐，并增加了一个注意力频道，专注于最能提供信息(触发)的单词。但是，必须谨慎地设置超参数，以控制触发单词的阈值，使其具有信息性。Huang等人使用端到端存储网络[30]处理相同的任务，该网络集成了一个分层的注意机制，超过了用户之前对单词和句子级别的关注。Chen等人将多媒体内容与代表推荐系统用户协作记忆网络ssigir 18、2018年7月8日至12日，密歇根州安娜堡，USApreferences和组件级注意相结合，以分离项目特定的视觉特征。类似地，Seo等人的[29]引入了一种局部和全局的关注机制来对文本进行建模。Xiaoet al.[38]利用注意机制扩展了因子分解机器[24]，以了解每一对交互的重要性，而不是对它们进行统一处理。现有的基于神经注意的方法大多依赖于附加内容或上下文信息，而我们的任务是研究协同过滤。据我们所知，在此之前的工作中，还没有使用内存网络架构来处理协同过滤集中的隐式反馈
 2.2记忆增强神经网络
        我们首先简要概述了基于内存的体系结构的内部工作原理。记忆增强神经网络一般由两部分组成:外部存储器(通常是一个ma-trix)和一个控制器(用于对存储器执行操作)。，读，写，擦)。内存组件增加了独立于控制器(通常是神经网络)的模型容量，同时提供了知识的内部表示，以跟踪长期依赖关系并执行推理。控制程序通过基于内容或基于位置的寻址操作这些内存。基于内容或关联寻址在给定的问题(查询)和一段文本之间找到一个评分函数，通常是内部积，然后是导致软读取每个内存位置的thesoftmax操作[1,18,30,35,39]。对内存位置执行软读取使模型保持差异化，因此可以通过反向传播进行训练。后一种寻址类型(通常与基于内容的寻址相结合)执行顺序读取或随机访问
        Weston等人提出的初始框架([35]demon-strated)显示出了很有希望的结果，可以跟踪对合成问题回答任务的长期依赖关系和每一种形式的推理。Sukhbaataret al.[30]减轻了将原始内存网络训练成端到端系统所需的强大监控级别。注意力的概念是由生物学上的原因引起的，即人类不是在一个给定的任务中统一地处理所有信息，而是专注于特定的信息子集。注意力机制还通过可视化注意力权重[1]，为深入学习黑箱提供了一定程度的洞察力。Kumar等人通过引入情景记忆组件改进了原有的体系结构，允许在生成最终答案之前对记忆进行多次传递或协商。记忆网络架构的灵活性使其能够执行可视化的问题回答[39]和联合任务学习，以识别情绪和与目标实体的关系
 2.3协同存储网络
        在本节中，我们将介绍我们所提议的模型CollaborativeMemory Network (CMN)，有关体系结构的可视化描述，请参见图1a。在高层，CMN维护三个内存状态:一个内部特定于用户的内存、一个特定于项的内存和一个集体邻域状态。该体系结构允许基于邻域方法的局部特殊结构与潜在因素模型的全局结构之间的联合非线性交互作用。关联寻址方案作为一个最近邻相似函数，学习根据当前项选择语义相似的用户。神经注意机制允许学习一个自适应非线性加权函数的邻居模型，其中最相似的用户贡献较高的输出模块的权重。稍后，我们通过在图1b中描述的第3.4节中叠加多个跃点，将模型扩展到更深层次的体系结构。
        该体系结构允许基于邻域方法的局部特殊结构与潜在因素模型的全局结构之间的联合非线性交互作用。关联寻址方案作为一个最近邻相似函数，学习根据当前项选择语义相似的用户。神经注意机制允许学习一个自适应非线性加权函数的邻居模型，其中最相似的用户贡献较高的输出模块的权重。稍后，我们通过在图1b中描述的第3.4节中叠加多个跃点，将模型扩展到更深层次的体系结构
        2.3.1内存组件由用户内存矩阵xm RP d和项内存矩阵xe RQ d组成，其中epandq分别表示用户和项的数量，并且d表示法表示每个内存单元的大小(维数)。每个用户用户都嵌入一个内存slotmu，存储她的特定首选项。类似地，每个条目都响应另一个内存slotei来编码条目的特定属性。我们形成了一个用户偏好向量，其中每个维度都与目标用户的相似度有关quiv=mTumv+eTimv∀v∈N(i)(1),其中(i)表示为itemi提供隐式反馈的所有用户(邻居)的集合。我们想指出，outN(i)可以被替换或与r (i)结合来处理显式反馈的情况，其中(i)表示为itemi提供显式反馈的所有用户的集合。直觉是这样的，第一项计算目标用户和对其进行emi评级的用户之间的兼容性。第二学期介绍了userv支持itemi推荐的置信度。因此，关联寻址方案识别出目标用户与给定特定项的相邻用户之间相似性最高的内部记忆。
        2.3.2社区的关注
        神经注意机制学习一种自适应加权函数，将注意力集中在邻居中有影响力的用户子集上，从而得出排名分数。传统的邻域方法预先定义启发式加权函数，如Pearsoncorrelation或cos similarity，并要求指定考虑[26]的用户数量。虽然对邻域进行因子分解可以有效地解决这一问题，但它本质上仍然是线性的。我们不再需要学习整个邻域的权重函数，而是需要根据经验预先定义权重函数或要考虑的邻域数量。形式上，我们计算给定用户的注意权值，以推断每个用户对邻居的独特贡献的重要性:puiv=exp(quiv)Ík∈N(i)exp(quik)∀v∈N(i)它在邻域上产生一个分布。attention机制允许模型将注意力集中在或将更高权重的特定用户放在附近，而将更少的重要性放在可能不太相似的用户上。接下来，我们通过用注意权值插值外部邻域记忆来构造最终的邻域表示:
        
       
